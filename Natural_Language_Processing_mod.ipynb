{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Natural Language Processing\n",
    "<!-- requirement: small_data/apple_fruit.txt -->\n",
    "<!-- requirement: small_data/apple_inc.txt -->\n",
    "<!-- requirement: small_data/ford_car.txt -->\n",
    "<!-- requirement: small_data/ford_crossing.txt -->\n",
    "<!-- requirement: small_data/window_glass.txt -->\n",
    "<!-- requirement: small_data/windows_ms.txt -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Natural language processing (NLP) is concerned with developing algorithms that enable computers to process or understand human (or \"natural\") language. In this notebook we will focus on using natural language processing to classify texts into different genres. \n",
    "\n",
    "**EXAMPLE:**\n",
    "In a given source block of text, we may want to differentiate Apple (the company) vs. apple (the fruit) using supervised classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Text as a \"bag of words\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We can imagine that documents belonging to the same class will share more words in common with each other than with documents belonging to different classes. This idea is key to how we will construct our classifier. As such, as a first step, we will keep track of the word counts (or frequencies) for each document. \n",
    "\n",
    "  - Split the text into words\n",
    "  - Count how many times each word (in some fixed vocabulary) occurs\n",
    "  - _(Optionally)_ normalize the counts against some baseline\n",
    "  - _(Variant)_ Just do a binary \"yes / no\" for whether each word (in some vocabulary) is contained in the material\n",
    "  \n",
    "Let us count the words we see in documents (in this case, sentences) pertaining to apples (the fruit) and Apple (the company). Our first step will be to pull in the training (and test) data. We will want to clean both data on the way in: our goal is to have each text as a list of strings, one string for each sentence. We'll be using `spaCy` for this. It should already be installed, and its data set downloaded, on your box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "When `spaCy` loads a document, it automatically tokenizes it into sentences and words. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a text document.\n",
      "spaCy requires it to be a unicode string.\n",
      "Word 3: text\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Here is a text document. spaCy requires it to be a unicode string.')\n",
    "# doc.sents is a generator producing sentences\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n",
    "# doc can be indexed to find the individual words\n",
    "print(\"Word 3:\", doc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now let's grab our documents from the \"apple\" and \"Apple\" Wikipedia pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Spit out (slightly cleaned up) sentences from a Wikipedia article.\n",
    "def wikipedia_to_sents(url):\n",
    "    soup = BeautifulSoup(urlopen(url), 'lxml').find(attrs={'id':'mw-content-text'})\n",
    "    \n",
    "    # The text is littered by references like [n].  Drop them.\n",
    "    def drop_refs(s):\n",
    "        return ''.join(re.split('\\[\\d+\\]', s))\n",
    "    \n",
    "    paragraphs = [drop_refs(p.text) for p in soup.find_all('p')]\n",
    "    return [s.text for paragraph in paragraphs for s in nlp(paragraph).sents if len(s) > 2]\n",
    "\n",
    "# Articles are pinned to a specific version for consistent results\n",
    "fruit_sents = wikipedia_to_sents(\"https://en.wikipedia.org/?title=Apple&oldid=1069655337\")\n",
    "company_sents = wikipedia_to_sents(\"https://en.wikipedia.org/?title=Apple_Inc.&oldid=1070267868\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple was ranked No. 4 on the 2018 Fortune 500 rankings of the largest United States corporations by total revenue.',\n",
       " 'Apple has created subsidiaries in low-tax places such as Ireland, the Netherlands, Luxembourg, and the British Virgin Islands to cut the taxes it pays around the world.',\n",
       " 'According to The New York Times, in the 1980s Apple was among the first tech companies to designate overseas salespeople in high-tax countries in a manner that allowed the company to sell on behalf of low-tax subsidiaries on other continents, sidestepping income taxes.',\n",
       " 'In the late 1980s, Apple was a pioneer of an accounting technique known as the \"Double Irish with a Dutch sandwich,\" which reduces taxes by routing profits through Irish subsidiaries and the Netherlands and then to the Caribbean.',\n",
       " 'British Conservative Party Member of Parliament Charlie Elphicke published research on October 30, 2012, which showed that some multinational companies, including Apple Inc., were making billions of pounds of profit in the UK, but were paying an effective tax rate to the UK Treasury of only 3 percent, well below standard corporation tax.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_sents[-105:-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['However, more than with most perennial fruits, apples must be propagated asexually to obtain the sweetness and other desirable characteristics of the parent.',\n",
       " 'This is because seedling apples are an example of \"extreme heterozygotes\", in that rather than inheriting genes from their parents to create a new apple with parental characteristics, they are instead significantly different from their parents, perhaps to compete with the many pests.',\n",
       " 'Triploid cultivars have an additional reproductive barrier in that three sets of chromosomes cannot be divided evenly during meiosis, yielding unequal segregation of the chromosomes (aneuploids).',\n",
       " 'Even in the case when a triploid plant can produce a seed (apples are an example), it occurs infrequently, and seedlings rarely survive.',\n",
       " 'Because apples are not true breeders when planted as seeds, although cuttings can take root and breed true, and may live for a century, grafting is usually used.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruit_sents[-105:-100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Count Vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Machine learning algorithms, however, prefer vectors of numbers, not text. When we do this translation, the output will be typically be a very large, but usually sparse, vector: The number of coordinates is the number of words in our dictionary, and the $i$-th coordinate entry is the number of occurrences of the $i$-th word.\n",
    "\n",
    "There's a reasonable implementation of this in the `CountVectorizer` class in `sklearn.feature_extraction.text`. See http://scikit-learn.org/stable/modules/classes.html#text-feature-extraction-ref for more detail on the options. When we use sklearn's `CountVectorizer`, we generate a matrix where each column corresponds to a word, each row corresponds to a document, and the values are the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(867, 4377)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_words_vectorizer = CountVectorizer()\n",
    "\n",
    "counts = bag_of_words_vectorizer.fit_transform( fruit_sents + company_sents  )\n",
    "print(counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "  (0, 379)\t3\n",
      "  (0, 429)\t2\n",
      "  (0, 2220)\t1\n",
      "  (0, 1402)\t1\n",
      "  (0, 1764)\t1\n",
      "  (0, 3101)\t1\n",
      "  (0, 715)\t1\n",
      "  (0, 4074)\t1\n",
      "  (0, 2475)\t1\n",
      "  (0, 1320)\t1\n",
      "  (1, 429)\t1\n",
      "  (1, 2475)\t1\n",
      "  (1, 4075)\t1\n",
      "  (1, 453)\t2\n",
      "  (1, 1128)\t1\n",
      "  (1, 4332)\t1\n",
      "  (1, 391)\t1\n",
      "  (1, 3968)\t2\n",
      "  (1, 2606)\t1\n",
      "  (1, 4293)\t1\n",
      "  (1, 1905)\t1\n",
      "  (1, 3722)\t1\n",
      "  (1, 2066)\t1\n",
      "  (1, 1821)\t1\n",
      "  (2, 2220)\t1\n",
      "  :\t:\n",
      "  (865, 2066)\t1\n",
      "  (865, 4020)\t1\n",
      "  (865, 1919)\t1\n",
      "  (865, 2099)\t1\n",
      "  (865, 1442)\t1\n",
      "  (865, 1070)\t1\n",
      "  (865, 1862)\t1\n",
      "  (865, 4329)\t1\n",
      "  (865, 4140)\t1\n",
      "  (865, 953)\t1\n",
      "  (865, 3460)\t1\n",
      "  (865, 3997)\t1\n",
      "  (865, 469)\t1\n",
      "  (866, 429)\t1\n",
      "  (866, 2066)\t1\n",
      "  (866, 2755)\t2\n",
      "  (866, 2838)\t1\n",
      "  (866, 3944)\t1\n",
      "  (866, 2247)\t1\n",
      "  (866, 102)\t1\n",
      "  (866, 3849)\t1\n",
      "  (866, 2898)\t1\n",
      "  (866, 1490)\t1\n",
      "  (866, 3446)\t1\n",
      "  (866, 162)\t1\n"
     ]
    }
   ],
   "source": [
    "# Note that counts is a **sparse** matrix.\n",
    "print(counts.toarray())       #This is what it actually looks like.. there are non-zero entries, really!\n",
    "print()\n",
    "print(counts)                 # .. this is just describing the non-zero entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['above', 'abroad', 'absence', 'absent', 'absolutely', 'abuse',\n",
       "       'abuses', 'abusive', 'acceptable', 'acceptance', 'access',\n",
       "       'accessible', 'accessions', 'accessories', 'accessory', 'acclaim',\n",
       "       'acclaimed', 'acclimatized', 'accomplish', 'according', 'account',\n",
       "       'accountability', 'accounted', 'accounting', 'accounts',\n",
       "       'achieves', 'acid', 'acidity', 'acknowledge', 'acknowledges',\n",
       "       'acquire', 'acquired', 'acquiring', 'acquisition', 'acquisitions',\n",
       "       'acres', 'across', 'act', 'acted', 'acting', 'action', 'actions',\n",
       "       'active', 'actively'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change by sunil\n",
    "bag_of_words_vectorizer.get_feature_names_out()[200:244]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'an': 379,\n",
       " 'apple': 429,\n",
       " 'is': 2220,\n",
       " 'edible': 1402,\n",
       " 'fruit': 1764,\n",
       " 'produced': 3101,\n",
       " 'by': 715,\n",
       " 'tree': 4074,\n",
       " 'malus': 2475,\n",
       " 'domestica': 1320,\n",
       " 'trees': 4075,\n",
       " 'are': 453,\n",
       " 'cultivated': 1128,\n",
       " 'worldwide': 4332,\n",
       " 'and': 391,\n",
       " 'the': 3968,\n",
       " 'most': 2606,\n",
       " 'widely': 4293,\n",
       " 'grown': 1905,\n",
       " 'species': 3722,\n",
       " 'in': 2066,\n",
       " 'genus': 1821,\n",
       " 'originated': 2820,\n",
       " 'central': 791,\n",
       " 'asia': 479,\n",
       " 'where': 4280,\n",
       " 'its': 2236,\n",
       " 'wild': 4295,\n",
       " 'ancestor': 388,\n",
       " 'sieversii': 3615,\n",
       " 'still': 3797,\n",
       " 'found': 1738,\n",
       " 'today': 4021,\n",
       " 'apples': 432,\n",
       " 'have': 1946,\n",
       " 'been': 582,\n",
       " 'for': 1716,\n",
       " 'thousands': 3996,\n",
       " 'of': 2755,\n",
       " 'years': 4356,\n",
       " 'europe': 1511,\n",
       " 'were': 4274,\n",
       " 'brought': 686,\n",
       " 'to': 4020,\n",
       " 'north': 2712,\n",
       " 'america': 370,\n",
       " 'european': 1512,\n",
       " 'colonists': 899,\n",
       " 'religious': 3311,\n",
       " 'mythological': 2646,\n",
       " 'significance': 3618,\n",
       " 'many': 2488,\n",
       " 'cultures': 1131,\n",
       " 'including': 2076,\n",
       " 'norse': 2711,\n",
       " 'greek': 1883,\n",
       " 'christian': 832,\n",
       " 'tradition': 4053,\n",
       " 'from': 1760,\n",
       " 'seed': 3526,\n",
       " 'tend': 3953,\n",
       " 'be': 569,\n",
       " 'very': 4208,\n",
       " 'different': 1257,\n",
       " 'those': 3991,\n",
       " 'parents': 2868,\n",
       " 'resultant': 3383,\n",
       " 'frequently': 1754,\n",
       " 'lack': 2314,\n",
       " 'desired': 1225,\n",
       " 'characteristics': 810,\n",
       " 'generally': 1806,\n",
       " 'then': 3974,\n",
       " 'cultivars': 1127,\n",
       " 'propagated': 3136,\n",
       " 'clonal': 864,\n",
       " 'grafting': 1868,\n",
       " 'onto': 2780,\n",
       " 'rootstocks': 3440,\n",
       " 'without': 4319,\n",
       " 'larger': 2328,\n",
       " 'much': 2627,\n",
       " 'slower': 3666,\n",
       " 'after': 292,\n",
       " 'planting': 2982,\n",
       " 'used': 4172,\n",
       " 'control': 1033,\n",
       " 'speed': 3730,\n",
       " 'growth': 1906,\n",
       " 'size': 3649,\n",
       " 'resulting': 3385,\n",
       " 'allowing': 346,\n",
       " 'easier': 1383,\n",
       " 'harvesting': 1941,\n",
       " 'there': 3975,\n",
       " 'more': 2604,\n",
       " 'than': 3966,\n",
       " '500': 154,\n",
       " 'known': 2304,\n",
       " 'bred': 671,\n",
       " 'various': 4196,\n",
       " 'tastes': 3933,\n",
       " 'use': 4171,\n",
       " 'cooking': 1043,\n",
       " 'eating': 1389,\n",
       " 'raw': 3220,\n",
       " 'cider': 835,\n",
       " 'production': 3106,\n",
       " 'prone': 3134,\n",
       " 'number': 2731,\n",
       " 'fungal': 1782,\n",
       " 'bacterial': 547,\n",
       " 'pest': 2938,\n",
       " 'problems': 3091,\n",
       " 'which': 4283,\n",
       " 'can': 738,\n",
       " 'controlled': 1034,\n",
       " 'organic': 2811,\n",
       " 'non': 2705,\n",
       " 'means': 2523,\n",
       " '2010': 90,\n",
       " 'genome': 1819,\n",
       " 'was': 4252,\n",
       " 'sequenced': 3552,\n",
       " 'as': 476,\n",
       " 'part': 2872,\n",
       " 'research': 3358,\n",
       " 'on': 2774,\n",
       " 'disease': 1290,\n",
       " 'selective': 3539,\n",
       " 'breeding': 674,\n",
       " '2018': 98,\n",
       " '86': 184,\n",
       " 'million': 2571,\n",
       " 'tonnes': 4027,\n",
       " 'with': 4315,\n",
       " 'china': 825,\n",
       " 'accounting': 223,\n",
       " 'nearly': 2666,\n",
       " 'half': 1923,\n",
       " 'total': 4036,\n",
       " 'word': 4323,\n",
       " 'formerly': 1731,\n",
       " 'spelled': 3731,\n",
       " 'æppel': 4374,\n",
       " 'old': 2770,\n",
       " 'english': 1462,\n",
       " 'derived': 1217,\n",
       " 'proto': 3150,\n",
       " 'germanic': 1825,\n",
       " 'root': 3438,\n",
       " 'ap': 416,\n",
       " 'laz': 2352,\n",
       " 'could': 1065,\n",
       " 'also': 355,\n",
       " 'mean': 2521,\n",
       " 'general': 1805,\n",
       " 'this': 3988,\n",
       " 'ultimately': 4114,\n",
       " 'indo': 2102,\n",
       " 'ab': 194,\n",
       " 'but': 711,\n",
       " 'precise': 3046,\n",
       " 'original': 2817,\n",
       " 'meaning': 2522,\n",
       " 'relationship': 3297,\n",
       " 'between': 609,\n",
       " 'both': 650,\n",
       " 'words': 4324,\n",
       " 'clarification': 846,\n",
       " 'needed': 2670,\n",
       " 'uncertain': 4117,\n",
       " 'late': 2335,\n",
       " '17th': 41,\n",
       " 'century': 793,\n",
       " 'functioned': 1775,\n",
       " 'generic': 1812,\n",
       " 'term': 3956,\n",
       " 'all': 333,\n",
       " 'other': 2824,\n",
       " 'berries': 606,\n",
       " 'nuts': 2738,\n",
       " 'such': 3847,\n",
       " '14th': 30,\n",
       " 'middle': 2561,\n",
       " 'appel': 428,\n",
       " 'paradis': 2863,\n",
       " 'banana': 551,\n",
       " 'analogous': 380,\n",
       " 'french': 1753,\n",
       " 'language': 2321,\n",
       " 'pomme': 3010,\n",
       " 'deciduous': 1179,\n",
       " 'standing': 3767,\n",
       " '15': 31,\n",
       " 'ft': 1768,\n",
       " 'tall': 3925,\n",
       " 'cultivation': 1129,\n",
       " 'up': 4155,\n",
       " '30': 122,\n",
       " 'when': 4279,\n",
       " 'shape': 3580,\n",
       " 'branch': 660,\n",
       " 'density': 1210,\n",
       " 'determined': 1233,\n",
       " 'rootstock': 3439,\n",
       " 'selection': 3538,\n",
       " 'trimming': 4085,\n",
       " 'method': 2549,\n",
       " 'leaves': 2367,\n",
       " 'alternately': 356,\n",
       " 'arranged': 466,\n",
       " 'dark': 1155,\n",
       " 'green': 1884,\n",
       " 'colored': 901,\n",
       " 'simple': 3630,\n",
       " 'ovals': 2837,\n",
       " 'serrated': 3557,\n",
       " 'margins': 2492,\n",
       " 'slightly': 3661,\n",
       " 'downy': 1341,\n",
       " 'undersides': 4122,\n",
       " 'blossoms': 636,\n",
       " 'spring': 3749,\n",
       " 'simultaneously': 3633,\n",
       " 'budding': 696,\n",
       " 'spurs': 3751,\n",
       " 'some': 3690,\n",
       " 'long': 2424,\n",
       " 'shoots': 3596,\n",
       " 'cm': 872,\n",
       " 'flowers': 1701,\n",
       " 'white': 4285,\n",
       " 'pink': 2963,\n",
       " 'tinge': 4015,\n",
       " 'that': 3967,\n",
       " 'gradually': 1865,\n",
       " 'fades': 1602,\n",
       " 'five': 1687,\n",
       " 'petaled': 2942,\n",
       " 'inflorescence': 2111,\n",
       " 'consisting': 1000,\n",
       " 'cyme': 1149,\n",
       " 'flower': 1699,\n",
       " 'called': 727,\n",
       " 'king': 2296,\n",
       " 'bloom': 634,\n",
       " 'it': 2229,\n",
       " 'opens': 2785,\n",
       " 'first': 1683,\n",
       " 'develop': 1235,\n",
       " 'pome': 3009,\n",
       " 'matures': 2515,\n",
       " 'summer': 3867,\n",
       " 'or': 2801,\n",
       " 'autumn': 530,\n",
       " 'exist': 1551,\n",
       " 'wide': 4292,\n",
       " 'range': 3205,\n",
       " 'sizes': 3650,\n",
       " 'commercial': 917,\n",
       " 'growers': 1903,\n",
       " 'aim': 309,\n",
       " 'produce': 3100,\n",
       " 'diameter': 1246,\n",
       " 'due': 1362,\n",
       " 'market': 2496,\n",
       " 'preference': 3051,\n",
       " 'consumers': 1010,\n",
       " 'especially': 1497,\n",
       " 'japan': 2248,\n",
       " 'prefer': 3050,\n",
       " 'while': 4284,\n",
       " 'below': 598,\n",
       " 'making': 2471,\n",
       " 'juice': 2266,\n",
       " 'little': 2407,\n",
       " 'fresh': 1755,\n",
       " 'value': 4187,\n",
       " 'skin': 3653,\n",
       " 'ripe': 3419,\n",
       " 'red': 3260,\n",
       " 'yellow': 4357,\n",
       " 'russetted': 3458,\n",
       " 'though': 3992,\n",
       " 'bi': 613,\n",
       " 'tri': 4078,\n",
       " 'may': 2517,\n",
       " 'wholly': 4289,\n",
       " 'partly': 2878,\n",
       " 'russeted': 3456,\n",
       " 'rough': 3441,\n",
       " 'brown': 687,\n",
       " 'covered': 1081,\n",
       " 'protective': 3145,\n",
       " 'layer': 2349,\n",
       " 'epicuticular': 1483,\n",
       " 'wax': 4259,\n",
       " 'exocarp': 1554,\n",
       " 'flesh': 1694,\n",
       " 'pale': 2858,\n",
       " 'yellowish': 4358,\n",
       " 'exocarps': 1555,\n",
       " 'occur': 2748,\n",
       " 'growing': 1904,\n",
       " 'mountains': 2617,\n",
       " 'southern': 3708,\n",
       " 'kazakhstan': 2278,\n",
       " 'kyrgyzstan': 2306,\n",
       " 'tajikistan': 3916,\n",
       " 'northwestern': 2715,\n",
       " 'likely': 2393,\n",
       " 'beginning': 587,\n",
       " 'forested': 1725,\n",
       " 'flanks': 1692,\n",
       " 'tian': 4006,\n",
       " 'shan': 3579,\n",
       " 'progressed': 3121,\n",
       " 'over': 2838,\n",
       " 'period': 2923,\n",
       " 'time': 4013,\n",
       " 'permitted': 2928,\n",
       " 'secondary': 3517,\n",
       " 'introgression': 2185,\n",
       " 'genes': 1813,\n",
       " 'into': 2180,\n",
       " 'open': 2781,\n",
       " 'pollinated': 3003,\n",
       " 'seeds': 3529,\n",
       " 'significant': 3619,\n",
       " 'exchange': 1542,\n",
       " 'sylvestris': 3899,\n",
       " 'crabapple': 1087,\n",
       " 'resulted': 3384,\n",
       " 'current': 1137,\n",
       " 'populations': 3018,\n",
       " 'being': 592,\n",
       " 'related': 3295,\n",
       " 'crabapples': 1088,\n",
       " 'morphologically': 2605,\n",
       " 'similar': 3627,\n",
       " 'progenitor': 3115,\n",
       " 'strains': 3807,\n",
       " 'recent': 3240,\n",
       " 'admixture': 264,\n",
       " 'contribution': 1031,\n",
       " 'latter': 2340,\n",
       " 'predominates': 3049,\n",
       " 'diploid': 1268,\n",
       " 'triploid': 4086,\n",
       " 'not': 2717,\n",
       " 'uncommon': 4119,\n",
       " 'has': 1943,\n",
       " '17': 38,\n",
       " 'chromosomes': 834,\n",
       " 'estimated': 1502,\n",
       " 'approximately': 444,\n",
       " '650': 167,\n",
       " 'mb': 2518,\n",
       " 'several': 3573,\n",
       " 'whole': 4287,\n",
       " 'sequences': 3553,\n",
       " 'made': 2458,\n",
       " 'available': 532,\n",
       " 'one': 2776,\n",
       " 'based': 563,\n",
       " 'cultivar': 1126,\n",
       " 'golden': 1855,\n",
       " 'delicious': 1201,\n",
       " 'however': 2008,\n",
       " 'sequence': 3551,\n",
       " 'turned': 4101,\n",
       " 'out': 2829,\n",
       " 'contain': 1013,\n",
       " 'errors': 1495,\n",
       " 'owing': 2844,\n",
       " 'high': 1976,\n",
       " 'degree': 1195,\n",
       " 'heterozygosity': 1973,\n",
       " 'combination': 907,\n",
       " 'ancient': 390,\n",
       " 'duplication': 1365,\n",
       " 'complicated': 952,\n",
       " 'assembly': 485,\n",
       " 'recently': 3241,\n",
       " 'double': 1335,\n",
       " 'trihaploid': 4083,\n",
       " 'individuals': 2101,\n",
       " 'yielding': 4362,\n",
       " 'higher': 1977,\n",
       " 'quality': 3183,\n",
       " 'around': 465,\n",
       " '57': 160,\n",
       " '000': 1,\n",
       " 'support': 3876,\n",
       " 'moderate': 2592,\n",
       " 'estimates': 1503,\n",
       " '42': 141,\n",
       " '44': 145,\n",
       " '700': 172,\n",
       " 'protein': 3146,\n",
       " 'coding': 883,\n",
       " 'among': 374,\n",
       " 'things': 3982,\n",
       " 'availability': 531,\n",
       " 'provided': 3155,\n",
       " 'evidence': 1529,\n",
       " 're': 3221,\n",
       " 'sequencing': 3554,\n",
       " 'multiple': 2635,\n",
       " 'accessions': 212,\n",
       " 'supported': 3877,\n",
       " 'suggesting': 3857,\n",
       " 'extensive': 1581,\n",
       " 'following': 1711,\n",
       " 'domestication': 1322,\n",
       " 'recognized': 3247,\n",
       " 'major': 2466,\n",
       " 'genetic': 1815,\n",
       " 'variability': 4192,\n",
       " 'region': 3283,\n",
       " 'considered': 999,\n",
       " 'center': 789,\n",
       " 'origin': 2816,\n",
       " 'thought': 3993,\n",
       " 'domesticated': 1321,\n",
       " '4000': 140,\n",
       " '10000': 7,\n",
       " 'ago': 301,\n",
       " 'travelled': 4071,\n",
       " 'along': 350,\n",
       " 'silk': 3625,\n",
       " 'road': 3426,\n",
       " 'hybridization': 2021,\n",
       " 'siberia': 3609,\n",
       " 'baccata': 541,\n",
       " 'caucasus': 779,\n",
       " 'orientalis': 2814,\n",
       " 'only': 2779,\n",
       " 'western': 4277,\n",
       " 'side': 3611,\n",
       " 'contributed': 1029,\n",
       " 'genetically': 1816,\n",
       " 'isolated': 2224,\n",
       " 'population': 3017,\n",
       " 'eastern': 1386,\n",
       " 'chinese': 826,\n",
       " 'soft': 3683,\n",
       " 'asiatica': 480,\n",
       " 'prunifolia': 3160,\n",
       " 'dessert': 1228,\n",
       " '2000': 79,\n",
       " 'these': 3979,\n",
       " 'hybrids': 2022,\n",
       " 'traits': 4061,\n",
       " 'selected': 3537,\n",
       " 'human': 2014,\n",
       " 'acidity': 227,\n",
       " 'color': 900,\n",
       " 'firmness': 1681,\n",
       " 'soluble': 3688,\n",
       " 'sugar': 3853,\n",
       " 'unusually': 4152,\n",
       " 'fruits': 1766,\n",
       " 'smaller': 3670,\n",
       " 'modern': 2593,\n",
       " 'at': 497,\n",
       " 'sammardenchia': 3476,\n",
       " 'cueis': 1124,\n",
       " 'site': 3643,\n",
       " 'near': 2664,\n",
       " 'udine': 4110,\n",
       " 'northeastern': 2713,\n",
       " 'italy': 2231,\n",
       " 'form': 1727,\n",
       " 'material': 2512,\n",
       " 'carbon': 755,\n",
       " 'dated': 1159,\n",
       " 'bce': 568,\n",
       " 'analysis': 381,\n",
       " 'yet': 4360,\n",
       " 'successfully': 3845,\n",
       " 'determine': 1232,\n",
       " 'whether': 4282,\n",
       " 'domesticus': 1323,\n",
       " 'containing': 1014,\n",
       " 'ancestry': 389,\n",
       " 'hard': 1934,\n",
       " 'distinguish': 1302,\n",
       " 'archeological': 448,\n",
       " 'record': 3251,\n",
       " 'foraged': 1717,\n",
       " 'plantations': 2980,\n",
       " 'indirect': 2097,\n",
       " 'third': 3985,\n",
       " 'millennium': 2570,\n",
       " 'east': 1385,\n",
       " 'substantial': 3840,\n",
       " 'classical': 850,\n",
       " 'antiquity': 412,\n",
       " 'certainly': 796,\n",
       " 'essential': 1498,\n",
       " 'able': 198,\n",
       " 'propagate': 3135,\n",
       " 'best': 607,\n",
       " 'unclear': 4118,\n",
       " 'invented': 2187,\n",
       " 'winter': 4309,\n",
       " 'picked': 2958,\n",
       " 'stored': 3803,\n",
       " 'just': 2271,\n",
       " 'above': 200,\n",
       " 'freezing': 1752,\n",
       " 'important': 2059,\n",
       " 'food': 1712,\n",
       " 'millennia': 2569,\n",
       " 'world': 4331,\n",
       " 'plants': 2983,\n",
       " 'spanish': 3712,\n",
       " 'introduced': 2182,\n",
       " 'chiloé': 824,\n",
       " 'archipelago': 449,\n",
       " '16th': 37,\n",
       " 'became': 577,\n",
       " 'particularly': 2877,\n",
       " 'well': 4272,\n",
       " 'adapted': 249,\n",
       " 'orchard': 2804,\n",
       " 'american': 371,\n",
       " 'continent': 1019,\n",
       " 'planted': 2981,\n",
       " 'boston': 649,\n",
       " 'reverend': 3404,\n",
       " 'william': 4298,\n",
       " 'blaxton': 630,\n",
       " '1625': 36,\n",
       " 'native': 2657,\n",
       " 'crab': 1086,\n",
       " 'once': 2775,\n",
       " 'common': 928,\n",
       " 'spread': 3746,\n",
       " 'trade': 4050,\n",
       " 'routes': 3443,\n",
       " 'colonial': 898,\n",
       " 'farms': 1624,\n",
       " '1845': 44,\n",
       " 'united': 4133,\n",
       " 'states': 3779,\n",
       " 'nursery': 2734,\n",
       " 'catalogue': 770,\n",
       " 'sold': 3687,\n",
       " '350': 129,\n",
       " 'showing': 3604,\n",
       " 'proliferation': 3126,\n",
       " 'new': 2687,\n",
       " 'early': 1376,\n",
       " '19th': 76,\n",
       " '20th': 104,\n",
       " 'irrigation': 2218,\n",
       " 'projects': 3125,\n",
       " 'washington': 4253,\n",
       " 'began': 585,\n",
       " 'allowed': 345,\n",
       " 'development': 1239,\n",
       " 'multibillion': 2630,\n",
       " 'dollar': 1318,\n",
       " 'industry': 2105,\n",
       " 'leading': 2359,\n",
       " 'product': 3105,\n",
       " 'until': 4150,\n",
       " 'farmers': 1623,\n",
       " 'frostproof': 1762,\n",
       " 'cellars': 788,\n",
       " 'during': 1366,\n",
       " 'their': 3969,\n",
       " 'own': 2845,\n",
       " 'sale': 3471,\n",
       " 'improved': 2063,\n",
       " 'transportation': 4069,\n",
       " 'train': 4059,\n",
       " 'replaced': 3338,\n",
       " 'necessity': 2668,\n",
       " 'storage': 3801,\n",
       " 'atmosphere': 501,\n",
       " 'facilities': 1594,\n",
       " 'keep': 2279,\n",
       " 'year': 4354,\n",
       " 'round': 3442,\n",
       " 'humidity': 2017,\n",
       " 'low': 2437,\n",
       " 'oxygen': 2850,\n",
       " 'dioxide': 1267,\n",
       " 'levels': 2383,\n",
       " 'maintain': 2464,\n",
       " 'freshness': 1757,\n",
       " 'they': 3981,\n",
       " '1960s': 51,\n",
       " 'mythology': 2647,\n",
       " 'goddess': 1849,\n",
       " 'iðunn': 2241,\n",
       " 'portrayed': 3022,\n",
       " 'prose': 3142,\n",
       " 'edda': 1399,\n",
       " 'written': 4342,\n",
       " '13th': 26,\n",
       " 'snorri': 3678,\n",
       " 'sturluson': 3829,\n",
       " 'providing': 3157,\n",
       " 'gods': 1851,\n",
       " 'give': 1833,\n",
       " 'them': 3970,\n",
       " 'eternal': 1506,\n",
       " 'youthfulness': 4369,\n",
       " 'scholar': 3502,\n",
       " 'ellis': 1426,\n",
       " 'davidson': 1162,\n",
       " 'links': 2402,\n",
       " 'practices': 3039,\n",
       " 'paganism': 2852,\n",
       " 'developed': 1236,\n",
       " 'she': 3587,\n",
       " 'points': 2996,\n",
       " 'buckets': 694,\n",
       " 'oseberg': 2823,\n",
       " 'ship': 3593,\n",
       " 'burial': 706,\n",
       " 'norway': 2716,\n",
       " 'having': 1947,\n",
       " 'described': 1219,\n",
       " 'transformed': 4062,\n",
       " 'nut': 2736,\n",
       " 'skáldskaparmál': 3655,\n",
       " 'graves': 1877,\n",
       " 'peoples': 2914,\n",
       " 'england': 1461,\n",
       " 'elsewhere': 1429,\n",
       " 'had': 1919,\n",
       " 'symbolic': 3901,\n",
       " 'symbol': 3900,\n",
       " 'fertility': 1646,\n",
       " 'southwest': 3709,\n",
       " 'notes': 2722,\n",
       " 'connection': 990,\n",
       " 'vanir': 4191,\n",
       " 'tribe': 4080,\n",
       " 'associated': 493,\n",
       " 'citing': 841,\n",
       " 'instance': 2147,\n",
       " 'eleven': 1424,\n",
       " 'given': 1834,\n",
       " 'woo': 4322,\n",
       " 'beautiful': 575,\n",
       " 'gerðr': 1827,\n",
       " 'skírnir': 3656,\n",
       " 'who': 4286,\n",
       " 'acting': 239,\n",
       " 'messenger': 2545,\n",
       " 'god': 1848,\n",
       " 'freyr': 1758,\n",
       " 'stanzas': 3768,\n",
       " '19': 45,\n",
       " '20': 77,\n",
       " 'skírnismál': 3657,\n",
       " 'further': 1783,\n",
       " 'chapter': 808,\n",
       " 'völsunga': 4237,\n",
       " 'saga': 3467,\n",
       " 'frigg': 1759,\n",
       " 'sends': 3546,\n",
       " 'rerir': 3357,\n",
       " 'he': 1950,\n",
       " 'prays': 3044,\n",
       " 'odin': 2754,\n",
       " 'child': 823,\n",
       " 'guise': 1915,\n",
       " 'crow': 1118,\n",
       " 'drops': 1357,\n",
       " 'his': 1985,\n",
       " 'lap': 2322,\n",
       " 'sits': 3645,\n",
       " 'atop': 502,\n",
       " 'mound': 2616,\n",
       " 'wife': 4294,\n",
       " 'consumption': 1012,\n",
       " 'results': 3386,\n",
       " 'six': 3647,\n",
       " 'pregnancy': 3053,\n",
       " 'birth': 625,\n",
       " 'caesarean': 720,\n",
       " 'section': 3520,\n",
       " 'son': 3694,\n",
       " 'hero': 1970,\n",
       " 'völsung': 4236,\n",
       " 'strange': 3808,\n",
       " 'phrase': 2953,\n",
       " 'hel': 1959,\n",
       " '11th': 15,\n",
       " 'poem': 2994,\n",
       " 'skald': 3651,\n",
       " 'thorbiorn': 3990,\n",
       " 'brúnarson': 691,\n",
       " 'imply': 2058,\n",
       " 'dead': 1166,\n",
       " 'potentially': 3030,\n",
       " 'nehalennia': 2677,\n",
       " 'sometimes': 3693,\n",
       " 'depicted': 1215,\n",
       " 'parallels': 2864,\n",
       " 'irish': 2215,\n",
       " 'stories': 3805,\n",
       " 'asserts': 486,\n",
       " 'northern': 2714,\n",
       " 'extends': 1580,\n",
       " 'back': 542,\n",
       " 'least': 2365,\n",
       " 'roman': 3432,\n",
       " 'empire': 1438,\n",
       " 'came': 731,\n",
       " 'varieties': 4194,\n",
       " 'small': 3669,\n",
       " 'bitter': 628,\n",
       " 'concludes': 977,\n",
       " 'figure': 1655,\n",
       " 'we': 4263,\n",
       " 'must': 2640,\n",
       " 'dim': 1265,\n",
       " 'reflection': 3274,\n",
       " 'guardian': 1909,\n",
       " 'life': 2389,\n",
       " 'giving': 1835,\n",
       " 'appear': 424,\n",
       " 'traditions': 4055,\n",
       " 'often': 2768,\n",
       " 'mystical': 2644,\n",
       " 'forbidden': 1719,\n",
       " 'identifying': 2036,\n",
       " 'religion': 3310,\n",
       " 'folktales': 1708,\n",
       " 'foreign': 1724,\n",
       " 'heracles': 1969,\n",
       " 'twelve': 4103,\n",
       " 'labours': 2313,\n",
       " 'required': 3355,\n",
       " 'travel': 4070,\n",
       " 'garden': 1797,\n",
       " 'hesperides': 1972,\n",
       " 'pick': 2957,\n",
       " 'off': 2756,\n",
       " 'discord': 1284,\n",
       " 'eris': 1491,\n",
       " 'disgruntled': 1292,\n",
       " 'excluded': 1545,\n",
       " 'wedding': 4268,\n",
       " 'peleus': 2910,\n",
       " 'thetis': 3980,\n",
       " 'retaliation': 3391,\n",
       " 'tossed': 4035,\n",
       " 'inscribed': 2133,\n",
       " 'καλλίστη': 4375,\n",
       " 'kalliste': 2273,\n",
       " 'transliterated': 4064,\n",
       " 'kallisti': 2274,\n",
       " 'party': 2883,\n",
       " 'three': 3999,\n",
       " 'goddesses': 1850,\n",
       " 'claimed': 843,\n",
       " 'hera': 1968,\n",
       " 'athena': 499,\n",
       " 'aphrodite': 419,\n",
       " 'paris': 2869,\n",
       " 'troy': 4091,\n",
       " 'appointed': 438,\n",
       " 'select': 3536,\n",
       " 'recipient': 3244,\n",
       " 'bribed': 675,\n",
       " 'tempted': 3951,\n",
       " 'him': 1981,\n",
       " 'woman': 4321,\n",
       " 'helen': 1961,\n",
       " 'sparta': 3713,\n",
       " 'awarded': 537,\n",
       " 'thus': 4005,\n",
       " 'indirectly': 2098,\n",
       " 'causing': 783,\n",
       " 'trojan': 4087,\n",
       " 'war': 4248,\n",
       " 'greece': 1882,\n",
       " 'sacred': 3462,\n",
       " 'throw': 4004,\n",
       " 'someone': 3691,\n",
       " 'symbolically': 3902,\n",
       " 'declare': 1182,\n",
       " 'love': 2435,\n",
       " 'similarly': 3629,\n",
       " 'catch': 772,\n",
       " 'show': 3602,\n",
       " 'acceptance': 209,\n",
       " 'epigram': 1484,\n",
       " 'claiming': 844,\n",
       " 'authorship': 528,\n",
       " 'plato': 2987,\n",
       " 'you': 4365,\n",
       " 'if': 2039,\n",
       " 'willing': 4300,\n",
       " 'me': 2520,\n",
       " 'take': 3917,\n",
       " 'share': 3582,\n",
       " 'your': 4367,\n",
       " 'girlhood': 1832,\n",
       " 'thoughts': 3995,\n",
       " 'what': 4278,\n",
       " 'pray': 3042,\n",
       " 'even': 1521,\n",
       " 'consider': 997,\n",
       " 'how': 2007,\n",
       " 'short': 3599,\n",
       " 'lived': 2409,\n",
       " 'beauty': 576,\n",
       " 'atalanta': 498,\n",
       " 'raced': 3196,\n",
       " 'her': 1967,\n",
       " 'suitors': 3863,\n",
       " 'attempt': 504,\n",
       " 'avoid': 535,\n",
       " 'marriage': 2503,\n",
       " 'outran': 2834,\n",
       " 'hippomenes': 1982,\n",
       " 'melanion': 2536,\n",
       " 'name': 2650,\n",
       " 'possibly': 3025,\n",
       " 'melon': 2537,\n",
       " 'defeated': 1191,\n",
       " 'cunning': 1133,\n",
       " 'knew': 2302,\n",
       " 'win': 4302,\n",
       " 'fair': 1608,\n",
       " 'race': 3195,\n",
       " 'so': 3680,\n",
       " 'gifts': 1830,\n",
       " 'distract': 1304,\n",
       " 'took': 4030,\n",
       " 'finally': 1668,\n",
       " 'successful': 3844,\n",
       " 'winning': 4307,\n",
       " 'hand': 1925,\n",
       " 'eden': 1400,\n",
       " 'book': 644,\n",
       " 'genesis': 1814,\n",
       " 'identified': 2035,\n",
       " 'popular': 3014,\n",
       " 'held': 1960,\n",
       " 'eve': 1520,\n",
       " 'coaxed': 881,\n",
       " 'adam': 248,\n",
       " 'identification': 2034,\n",
       " 'unknown': 4138,\n",
       " 'biblical': 614,\n",
       " 'times': 4014,\n",
       " 'confusion': 987,\n",
       " 'latin': 2339,\n",
       " 'mālum': 2648,\n",
       " 'mălum': 2649,\n",
       " 'evil': 1531,\n",
       " 'each': 1373,\n",
       " 'normally': 2709,\n",
       " 'malum': 2474,\n",
       " 'knowledge': 2303,\n",
       " 'good': 1858,\n",
       " 'inappropriate': 2067,\n",
       " 'external': 1582,\n",
       " 'link': 2401,\n",
       " 'bonum': 642,\n",
       " 'et': 1505,\n",
       " 'renaissance': 3324,\n",
       " 'painters': 2857,\n",
       " 'influenced': 2113,\n",
       " 'story': 3806,\n",
       " 'result': 3382,\n",
       " 'immortality': 2053,\n",
       " 'temptation': 3950,\n",
       " 'fall': 1612,\n",
       " 'man': 2477,\n",
       " 'sin': 3634,\n",
       " 'itself': 2237,\n",
       " 'larynx': 2331,\n",
       " 'throat': 4000,\n",
       " 'because': 578,\n",
       " 'notion': 2726,\n",
       " 'caused': 781,\n",
       " 'remaining': 3315,\n",
       " 'sexual': 3575,\n",
       " 'seduction': 3524,\n",
       " 'sexuality': 3576,\n",
       " 'ironic': 2217,\n",
       " 'vein': 4200,\n",
       " 'proverb': 3153,\n",
       " 'day': 1163,\n",
       " 'keeps': 2281,\n",
       " 'doctor': 1312,\n",
       " 'away': 540,\n",
       " 'addressing': 256,\n",
       " 'supposed': 3878,\n",
       " 'health': 1956,\n",
       " 'benefits': 603,\n",
       " 'traced': 4044,\n",
       " 'wales': 4241,\n",
       " 'eat': 1387,\n",
       " 'going': 1853,\n",
       " 'bed': 581,\n",
       " 'll': 2411,\n",
       " 'earning': 1377,\n",
       " 'bread': 667,\n",
       " 'evolved': 1532,\n",
       " 'no': 2701,\n",
       " 'pay': 2896,\n",
       " 'phrasing': 2954,\n",
       " 'now': 2729,\n",
       " 'commonly': 929,\n",
       " 'recorded': 3252,\n",
       " '1922': 47,\n",
       " 'despite': 1227,\n",
       " 'daily': 1150,\n",
       " 'any': 414,\n",
       " 'effects': 1411,\n",
       " 'vary': 4197,\n",
       " 'yield': 4361,\n",
       " 'ultimate': 4113,\n",
       " 'same': 3475,\n",
       " 'temperate': 3948,\n",
       " 'subtropical': 3841,\n",
       " 'climates': 862,\n",
       " 'uk': 4111,\n",
       " 'national': 2654,\n",
       " 'collection': 894,\n",
       " 'responsibility': 3376,\n",
       " 'department': 1212,\n",
       " 'environment': 1477,\n",
       " 'rural': 3454,\n",
       " 'affairs': 285,\n",
       " 'includes': 2075,\n",
       " 'kent': 2282,\n",
       " 'university': 4136,\n",
       " 'reading': 3227,\n",
       " 'responsible': 3377,\n",
       " 'developing': 1238,\n",
       " 'database': 1157,\n",
       " 'provides': 3156,\n",
       " 'access': 210,\n",
       " 'search': 3512,\n",
       " 'work': 4325,\n",
       " 'cooperative': 1046,\n",
       " 'programme': 3117,\n",
       " 'plant': 2979,\n",
       " 'resources': 3370,\n",
       " '38': 133,\n",
       " 'countries': 1069,\n",
       " 'participating': 2875,\n",
       " 'pyrus': 3178,\n",
       " 'group': 1898,\n",
       " 'contains': 1015,\n",
       " 'information': 2115,\n",
       " 'alternative': 357,\n",
       " 'names': 2652,\n",
       " 'essentially': 1499,\n",
       " 'specifically': 3724,\n",
       " 'producing': 3104,\n",
       " 'typically': 4109,\n",
       " 'too': 4029,\n",
       " 'tart': 3931,\n",
       " 'astringent': 496,\n",
       " 'beverage': 610,\n",
       " 'rich': 3411,\n",
       " 'flavor': 1693,\n",
       " 'cannot': 746,\n",
       " 'commercially': 918,\n",
       " 'crisp': 1105,\n",
       " 'desirable': 1224,\n",
       " 'qualities': 3182,\n",
       " 'colorful': 902,\n",
       " 'absence': 202,\n",
       " 'russeting': 3457,\n",
       " 'ease': 1382,\n",
       " 'shipping': 3594,\n",
       " 'lengthy': 2376,\n",
       " 'ability': 197,\n",
       " 'yields': 4363,\n",
       " 'resistance': 3368,\n",
       " 'sweeter': 3895,\n",
       " 'older': 2771,\n",
       " 'varied': 4193,\n",
       " 'americans': 372,\n",
       " 'europeans': 1513,\n",
       " 'favor': 1628,\n",
       " 'sweet': 3894,\n",
       " 'subacid': 3830,\n",
       " 'strong': 3819,\n",
       " 'minority': 2579,\n",
       " 'extremely': 1587,\n",
       " 'barely': 558,\n",
       " 'acid': 226,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sunil\n",
    "bag_of_words_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Hashing Vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "When doing \"bag of words\" type techniques on a *large* corpus and without an existing vocabulary, there is a simple trick that is often useful.  The issue (and solution) is as follows: \n",
    "\n",
    " - The output is a feature vector, so that whenever we encounter a word we must look up which coordinate slot it is in.  A naive way would be to keep a list of all the words encountered so far, and look up each word when it is encountered.  Whenever we encounter a new word, we see if we've already seen it before and if not -- assign it a new number.  This requires storing all the words that we have seen in memory, cannot be done in parallel (because we'd have to share the table of seen words), etc.\n",
    " - A **hash function** takes as input something complicated (like a string) and spits out a number, with the desired property being that different inputs *usually* produce different outputs.  (This is how hash tables are implemented, as the name suggests.)\n",
    " - So -- rather than exactly looking up the coordinate of a given word, we can just use its hash value (modulo a big size that we choose).  This is fast and parallelizes easily.  (There are some downsides: You cannot tell, after the fact, what word each of your feature actually corresponds to!)\n",
    " \n",
    "Scikit-learn includes `sklearn.feature_extraction.text.HashingVectorizer` to do this.  It behaves as almost a drop-in replacement for `CountVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Word importance: Term frequency–inverse document frequency (TF-IDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Just using counts (or frequencies), as above, is clearly not great.  Both apples, the fruit, and Apple, the company, are enjoyed around the world!  We would like to find words that are common in one document, but not common in all of them.  This is the goal of the __tf-idf weighting__.  A precise definition is:\n",
    "\n",
    "\n",
    "  1. If $d$ denotes a document and $t$ denotes a term, then the _raw term frequency_ $\\mathrm{tf}^{raw}(t,d)$ is\n",
    "  \n",
    "  $$ \\mathrm{tf}^{raw}(t,d) = \\text{the number of times the term $t$ occurs in the document $d$} $$\n",
    "  \n",
    "  The vector of all term frequencies can optionally be _normalized_ either by dividing by the maximum of any single word's occurrence count ($L^1$) or by the Euclidean length of the vector of word occurrence counts ($L^2$).  Scikit-learn by default does this second one:\n",
    "  \n",
    "  $$ \\mathrm{tf}(t,d) = \\mathrm{tf}^{L^2}(t,d) = \\frac{\\mathrm{tf}^{raw}(t,d)}{\\sqrt{\\sum_t \\mathrm{tf}^{raw}(t,d)^2}} $$\n",
    "  \n",
    "  (Scikit-learn actually does this normalization _after_ applying the $\\mathrm{idf}$ step below)\n",
    "  \n",
    "  2. If $$ D = \\left\\{ d : d \\in D \\right\\} $$ is the set of possible documents, then  the _inverse document frequency_ is\n",
    "  \n",
    "  $$ \\mathrm{idf}^{naive}(t,D) = \\log \\frac{\\# D}{\\# \\{d \\in D : t \\in d\\}} \\\\\n",
    "  = \\log \\frac{\\text{count of all documents}}{\\text{count of those documents containing the term $t$}} $$\n",
    "  \n",
    "  with a common variant being\n",
    "  \n",
    "  $$ \\mathrm{idf}(t, D) = \\log \\frac{1 + \\# D}{1 + \\# \\{d \\in D : t \\in d\\}} \\\\\n",
    "   = \\log \\frac{1 + \\text{count of all documents}}{1 + \\text{count of those documents containing the term $t$}} $$\n",
    "   \n",
    "  (Without the $1+$ in the denominator, we have to worry about dividing by zero if $t$ is not found in any documents). This will produce zero weight if a term appears in every document, so Scikit-learn actually uses\n",
    "  \n",
    "  $$ \\mathrm{idf}(t, D) = 1 + \\log \\frac{1+\\# D}{1 + \\# \\{d \\in D : t \\in d\\}}$$\n",
    "  \n",
    "  3. Finally, the weight that we assign to the term $t$ appearing in document $d$ and depending on the corpus of all documents $D$ is\n",
    "  \n",
    "  $$ \\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\mathrm{idf}(t,D) $$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The `CountVectorizer` and `HashingVectorizer` can be used to compute tf-idf values by combining them with the `TfidfTransformer` (the `TfidfVectorizer` is the `CountVectorizer` together with the `TfidfTransformer`). For our application (where the training and test data is small), we may as well just use `TfidfVectorizer` -- but it is good to know that `HashingVectorizer` is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fruit' 'generally' 'genome' 'golden' 'government']\n",
      "  (0, 265)\t0.3322776109544498\n",
      "  (0, 205)\t0.3751168070999027\n",
      "  (0, 158)\t0.36159994684483426\n",
      "  (0, 133)\t0.2083640248196821\n",
      "  (0, 100)\t0.287599162892657\n",
      "  (0, 56)\t0.1893017126779034\n",
      "  (0, 31)\t0.21340585183865515\n",
      "  (0, 26)\t0.6407951368214393\n",
      "  (1, 295)\t0.3429914934653169\n",
      "  (1, 266)\t0.33490142716129156\n",
      "  (1, 252)\t0.1833729444178797\n",
      "  (1, 171)\t0.2998147685178659\n",
      "  (1, 158)\t0.3687277536387121\n",
      "  (1, 118)\t0.11773514895097484\n",
      "  (1, 105)\t0.3752742454129184\n",
      "  (1, 72)\t0.3752742454129184\n",
      "  (1, 36)\t0.4356175996972143\n",
      "  (1, 31)\t0.10880623883994807\n",
      "  (1, 27)\t0.11661306278015948\n",
      "  (2, 287)\t0.37442789103849117\n",
      "  (2, 283)\t0.3609358550033812\n",
      "  (2, 265)\t0.3316673706807502\n",
      "  (2, 252)\t0.08974896766627086\n",
      "  (2, 242)\t0.3609358550033812\n",
      "  (2, 231)\t0.3673440073121736\n",
      "  :\t:\n",
      "  (864, 252)\t0.07884157365358642\n",
      "  (864, 244)\t0.25781194962619014\n",
      "  (864, 225)\t0.298751665090909\n",
      "  (864, 135)\t0.18381935337339275\n",
      "  (864, 118)\t0.10124093766505689\n",
      "  (864, 99)\t0.18670107242901937\n",
      "  (864, 83)\t0.2735053150589066\n",
      "  (864, 56)\t0.16599004170969284\n",
      "  (864, 31)\t0.09356293122414455\n",
      "  (864, 30)\t0.5635202959264894\n",
      "  (864, 26)\t0.18729468068669824\n",
      "  (864, 18)\t0.5325279796142962\n",
      "  (865, 263)\t0.19055326592111102\n",
      "  (865, 252)\t0.2761899051254341\n",
      "  (865, 118)\t0.1773285569576534\n",
      "  (865, 106)\t0.40962403645456874\n",
      "  (865, 104)\t0.5761247532493995\n",
      "  (865, 87)\t0.498823396954149\n",
      "  (865, 31)\t0.3277602906760159\n",
      "  (866, 249)\t0.5402099588532177\n",
      "  (866, 192)\t0.49429482382866985\n",
      "  (866, 181)\t0.381906043523013\n",
      "  (866, 137)\t0.4981338577963358\n",
      "  (866, 118)\t0.1941060975643581\n",
      "  (866, 31)\t0.1793852948763177\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "ng_tfidf = TfidfVectorizer(max_features=300)\n",
    "ng_tfidf.fit(fruit_sents + company_sents)\n",
    "print(ng_tfidf.get_feature_names_out()[100:105])\n",
    "print(ng_tfidf.transform(fruit_sents + company_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Limiting features\n",
    "\n",
    "Note that we used `max_features=300` here to limit how many terms the vectorizer returns, restricting it to the $300$ most common terms.  This works for the `CountVectorizer` as well, but not the `HashingVectorizer`.  \n",
    "\n",
    "We could also limit the number of terms using `min_df` and `max_df` (also for `TfidfVectorizer` and `CountVectorizer` but not `HashingVectorizer`). Here `min_df` indicates the minimum number of documents a term must appear in, and `max_df` the maximum.  `min_df=10`, for example, would reject any term that doesn't appear in at least $10$ documents, and `min_df=0.01` would reject any term that doesn't appear in at least $1\\%$ of documents - integers are interpreted as a fixed number of documents, floats as a fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['appears' 'more']\n"
     ]
    }
   ],
   "source": [
    "example_doc = [\"singleton appears once\",\n",
    "              \"once appears more than once than\",\n",
    "              \"once more\"]\n",
    "\n",
    "cv = CountVectorizer(min_df=2, max_df=0.8)\n",
    "cv.fit(example_doc)\n",
    "\n",
    "print(cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Here, `singleton` and `than` are rejected because they only appear in one document (even though `than` appears twice in its document), and `once` is rejected since it appears in every document, leaving only `appears` and `more`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Document similarity metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "A common problem is looking up a document similar to a given snippet, or relatedly comparing two documents for similarity.  The above provides a simple method for this called __cosine similarity__:\n",
    "  - To each of the two documents $d_1, d_2$ in a corpus of documents $D$, assign its tf or tf-idf vector $$ (v_i)_{j} = \\mathrm{tfidf}( t_{j}, d_i, D ) $$\n",
    "  where $i$ ranges over indices for documents, and $j$ ranges over indices for terms in the vocabulary.\n",
    "  - To compare two documents, simply find the cosine of the angle between the vectors:\n",
    "  $$ \\frac{v_i \\cdot v_{i'}}{|v_i| |v_{i'}|} $$\n",
    "  \n",
    "(There's also a variant using binary vectors and Jaccard distance.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Engineering your features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "There are many considerations to make when deciding which \"words\" or features to include in your classifier. In our example application, we might try to consider:\n",
    "   - Capitalization of the word apple? (`_a_pple` vs `_A_pple`)    \n",
    "   - Pluralization of the word apple? (apples)\n",
    "   - Possessive form of the word apple? (Apple's)\n",
    "   - Presence (or frequency) of certain well-chosen words : Does (e.g.,) the word \"computer\" or \"fruit\" occur in the sentence?  (This feature regards the sentence as a simple __bag of words__ without regard to trying to parse its structure.)\n",
    "   - In addition to single words, we can also look for __n-grams__: Strings of n consecutive words.\n",
    "   - Using the __tf-idf__ values of words or n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "It's common to want to __omit__ certain common words when doing these counts -- \"a\", \"an\", and \"the\" are common enough so that their counts do not tend to give us any hints as to the meaning of documents.  Such words that we want to omit are called __stop words__. (Note that, if you are using tf-idf and not the word frequency to compute your document similarity, you don't need to exclude stop words.)\n",
    "\n",
    "`spaCy` contains a standard list of such stop words for English in `spacy.lang.en.stop_words.STOP_WORDS`.  In our application, we'd also want to include \"apple\" -- it is certainly not going to help us distinguish our two meanings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'hence',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " \"n't\",\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'n‘t',\n",
       " 'n’t',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '‘d',\n",
       " '‘ll',\n",
       " '‘m',\n",
       " '‘re',\n",
       " '‘s',\n",
       " '‘ve',\n",
       " '’d',\n",
       " '’ll',\n",
       " '’m',\n",
       " '’re',\n",
       " '’s',\n",
       " '’ve'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Removing a few words that don't lemmatize well\n",
    "STOP_WORDS = STOP_WORDS.difference({'he','his','her','hers'})\n",
    "\n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The question of which words to include in your list of stop words is not always a straight forward one. A detail to keep in mind is that your list needs to use the same preprocessing and tokenization as your vectorizer. For instance, the stop words supplied by `spaCy` include several contractions (`\"'d\"`, `\"'ll\"`, etc.) with the apostrophe `'` included. Because `scikit-learn` handles these contractions [a little differently](https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words), we need to add a few contractions *without* the apostrophe to our list of stop words. Otherwise, `scikit-learn` will give us a `UserWarning` to let us know that we need to tweak our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "STOP_WORDS = STOP_WORDS.union({'ll', 've'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Another thing to look out for is whether your list of stop words might lead you to exclude words that are actually valuable in the context of your project. It is worth checking if the stop word list includes words that might be important for you.  Some of the choices may be idiosyncratic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False True\n"
     ]
    }
   ],
   "source": [
    "print('six' in STOP_WORDS, 'seven' in STOP_WORDS, 'eight' in STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Once stop words have been removed, we might hope that the most-frequently-used remaining words are the important features to keep.  For instance, here we'll compute and then use the top 300 words by frequency, *ignoring* the stop words from above.  Since \"apple\" is not useful in distinguishing the meanings, but will be common, we add it as a stop word.\n",
    "\n",
    "Nevertheless, this method is probably not as good as taking the top words as determined by tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000' '10' '100' '13' '14' '19' '1984' '1985' '1997' '20' '2001' '2006'\n",
      " '2007' '2008' '2009' '2010' '2011' '2012' '2013' '2014' '2015' '2016'\n",
      " '2017' '2018' '2019' '2020' '2021' '30' '500' 'according' 'allowed'\n",
      " 'america' 'american' 'announced' 'app' 'apples' 'applications' 'apps'\n",
      " 'april' 'asia' 'audio' 'august' 'away' 'based' 'began' 'best' 'billion'\n",
      " 'board' 'brand' 'business' 'called' 'came' 'campus' 'carbon' 'central'\n",
      " 'century' 'ceo' 'china' 'cider' 'city' 'climate' 'companies' 'company'\n",
      " 'computer' 'computers' 'conditions' 'consumer' 'consumers' 'content'\n",
      " 'continues' 'cook' 'corporation' 'cost' 'created' 'criticized' 'cultivar'\n",
      " 'cultivars' 'cultivated' 'data' 'davidson' 'day' 'december' 'design'\n",
      " 'designed' 'desktop' 'despite' 'development' 'device' 'devices'\n",
      " 'different' 'digital' 'early' 'efforts' 'electronics' 'employees' 'end'\n",
      " 'energy' 'environmental' 'europe' 'european' 'executives' 'facebook'\n",
      " 'following' 'fortune' 'found' 'foxconn' 'free' 'fruit' 'generally'\n",
      " 'generation' 'genome' 'glass' 'golden' 'google' 'government' 'green'\n",
      " 'group' 'grown' 'having' 'he' 'health' 'help' 'high' 'higher' 'his'\n",
      " 'human' 'ii' 'imac' 'inc' 'include' 'including' 'individuals' 'industry'\n",
      " 'information' 'intel' 'interface' 'introduced' 'introducing' 'ios' 'ipad'\n",
      " 'iphone' 'ipod' 'irish' 'itunes' 'january' 'jobs' 'juice' 'july' 'june'\n",
      " 'known' 'labor' 'large' 'largest' 'late' 'later' 'launched' 'led' 'life'\n",
      " 'like' 'line' 'lisa' 'local' 'located' 'logo' 'low' 'lower' 'mac'\n",
      " 'macbook' 'macintosh' 'macos' 'major' 'making' 'malus' 'manufacturer'\n",
      " 'manufacturing' 'march' 'market' 'media' 'microsoft' 'million' 'models'\n",
      " 'modern' 'months' 'music' 'named' 'national' 'new' 'north' 'november'\n",
      " 'number' 'october' 'opened' 'operating' 'operations' 'organic' 'original'\n",
      " 'os' 'pay' 'people' 'period' 'personal' 'pests' 'platform' 'points'\n",
      " 'policy' 'popular' 'power' 'practices' 'price' 'privacy' 'pro' 'problems'\n",
      " 'produce' 'produced' 'product' 'production' 'products' 'profit' 'profits'\n",
      " 'program' 'public' 'publicly' 'purchase' 'quality' 'raw' 'received'\n",
      " 'record' 'red' 'release' 'released' 'renewable' 'replaced' 'report'\n",
      " 'reported' 'research' 'retail' 'revenue' 'rights' 'rootstocks' 'said'\n",
      " 'sales' 'sculley' 'security' 'seeds' 'sell' 'september' 'series'\n",
      " 'service' 'services' 'share' 'sieversii' 'significant' 'siri' 'size'\n",
      " 'small' 'software' 'sold' 'specifically' 'started' 'stated' 'states'\n",
      " 'stating' 'steve' 'stock' 'storage' 'store' 'stores' 'success'\n",
      " 'successful' 'support' 'system' 'tax' 'tech' 'technology' 'tim' 'time'\n",
      " 'times' 'took' 'total' 'touch' 'tree' 'trees' 'tv' 'uk' 'united' 'update'\n",
      " 'use' 'user' 'users' 'video' 'water' 'wild' 'work' 'workers' 'working'\n",
      " 'world' 'worldwide' 'wozniak' 'year' 'years']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<867x300 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3940 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = CountVectorizer(max_features=300,\n",
    "                          stop_words=STOP_WORDS.union({'apple'}))\n",
    "counter.fit(fruit_sents + company_sents)\n",
    "print(counter.get_feature_names_out())\n",
    "\n",
    "# Now we can use it with that vectorizer, like so...\n",
    "counter.transform(fruit_sents + company_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Tokenization refers to splitting the text into pieces, in this case into sentences and into words. Instead of looking at just single words, it is also useful to look at **n-grams**: These are n-word long sequences of words (i.e., each of \"farmer's market\", \"market share\", and \"farm share\" is a 2-gram).\n",
    "\n",
    "The exact same sort of counting techniques apply.  The `CountVectorizer` function has built in support for this, too:\n",
    "\n",
    "If you pass it the `ngram_range=(m, M)` then it will count $n$-grams with  $m \\leq n \\leq M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000 time' '000 units' '100 million' '100 renewable' '122 0090'\n",
      " '13 billion' '17th century' '19th century' '2011 jobs' '2016 update'\n",
      " '2017 announced' '2019 update' '2020 announced' '2021 update'\n",
      " '2022 update' '3349 122' '37 3349' '500 known' '500 list' '65 billion'\n",
      " 'according report' 'active use' 'advanced manufacturing'\n",
      " 'adverse reactions' 'advertising campaigns' 'aim alliance'\n",
      " 'allowed company' 'amazon echo' 'ancestor malus' 'announced away'\n",
      " 'announced billion' 'announced internal' 'announcement came'\n",
      " 'anti competitive' 'app devices' 'app store' 'app tracking' 'apps app'\n",
      " 'august 2018' 'backlit lcd' 'board directors' 'brand loyalty'\n",
      " 'carbon dioxide' 'central asia' 'ceo tim' 'chief operating'\n",
      " 'chinese government' 'climate counts' 'co founder' 'coca cola'\n",
      " 'collection database' 'commercial orchards' 'commonly known'\n",
      " 'companies time' 'company felt' 'company focus' 'company history'\n",
      " 'company market' 'company product' 'company products'\n",
      " 'company proprietary' 'company provide' 'company revenue'\n",
      " 'company started' 'competitive practices' 'computer company'\n",
      " 'computer inc' 'computer manufacturer' 'computer sold' 'computers use'\n",
      " 'concentrated frozen' 'consumer electronics' 'controlled atmosphere'\n",
      " 'cook announced' 'cultivars bred' 'data centers' 'davidson notes'\n",
      " 'days later' 'december 2017' 'desktop publishing' 'dessert apples'\n",
      " 'disease resistance' 'displays arsenic' 'doctor away' 'dominant position'\n",
      " 'donated million' 'double irish' 'dr dre' 'dwarf rootstocks'\n",
      " 'earlier generations' 'early 1990s' 'effective tax' 'efforts hurricane'\n",
      " 'egremont russet' 'electronics companies' 'electronics manufacturer'\n",
      " 'end 2021' 'end year' 'families production' 'fiscal year'\n",
      " 'forbidden fruit' 'form allergy' 'fortune 500' 'genome sequences'\n",
      " 'golden apples' 'golden delicious' 'granny smith' 'graphical user'\n",
      " 'grown rootstock' 'guy kawasaki' 'health effects' 'help prevent'\n",
      " 'hera athena' 'high end' 'high level' 'high sierra' 'high yields'\n",
      " 'higher price' 'higher quality' 'highly successful' 'his death'\n",
      " 'his health' 'houses 500' 'human rights' 'hundreds people' 'ibm motorola'\n",
      " 'ibook laptop' 'icloud cloud' 'ii division' 'imac hello' 'inch screen'\n",
      " 'information technology' 'intel processors' 'intellectual property'\n",
      " 'internal memo' 'ipad mini' 'iphone apps' 'iphone devices' 'iphone ipad'\n",
      " 'iphone iphone' 'iphone models' 'ipod portable' 'ipod touch' 'irish gdp'\n",
      " 'itunes store' 'january 2007' 'january 2020' 'january 2021'\n",
      " 'january 2022' 'january 27' 'jean louis' 'jobs announced' 'jobs resigned'\n",
      " 'jobs steve' 'jobs wozniak' 'john sculley' 'journal reported' 'june 2017'\n",
      " 'labor practices' 'largest mobile' 'later year' 'law enforcement'\n",
      " 'lcd displays' 'led backlit' 'level brand' 'life cycle'\n",
      " 'life threatening' 'list companies' 'located inside' 'location services'\n",
      " 'logo designed' 'long lines' 'louis gassée' 'low tax' 'lower cost'\n",
      " 'm2 campus' 'mac mini' 'mac os' 'mac pro' 'mac transition' 'macbook pro'\n",
      " 'macintosh computers' 'macintosh division' 'macintosh ii'\n",
      " 'macintosh operating' 'macos ios' 'major product' 'malus domestica'\n",
      " 'malus sieversii' 'malus sylvestris' 'manufacturing iphone' 'march 16'\n",
      " 'march 24' 'march 30' 'market cap' 'market capitalization' 'market share'\n",
      " 'market value' 'marketing company' 'middle east' 'mobile phone'\n",
      " 'multi touch' 'new york' 'norse mythology' 'north america'\n",
      " 'north carolina' 'northern europe' 'november 10' 'old cultivars'\n",
      " 'online store' 'operating officer' 'operating system' 'operations run'\n",
      " 'original macintosh' 'pc clones' 'personal assistant' 'personal computer'\n",
      " 'personal computers' 'phil schiller' 'phone manufacturer' 'pies cooked'\n",
      " 'polyphenol oxidase' 'position desktop' 'power mac' 'power macintosh'\n",
      " 'preliminary research' 'presentation company' 'president retail'\n",
      " 'preventing ripening' 'previous year' 'price points' 'product line'\n",
      " 'profit margins' 'publicly traded' 'publishing market' 'ranked world'\n",
      " 'record labels' 'released iphone' 'relief efforts' 'renewable energy'\n",
      " 'renewable sources' 'resulting tree' 'retail locations' 'retail market'\n",
      " 'retail stores' 'ridley scott' 'right policy' 'right repair'\n",
      " 'ron johnson' 'ronald wayne' 'rootstocks today' 'run renewable' 'said he'\n",
      " 'sales international' 'sales second' 'san francisco' 'second largest'\n",
      " 'senior vice' 'september 2020' 'shan mountains' 'sold his' 'steve jobs'\n",
      " 'steve wozniak' 'stock price' 'street journal' 'supply chain' 'tax rate'\n",
      " 'tech company' 'technology company' 'television advertisement'\n",
      " 'themed version' 'tian shan' 'tim cook' 'time company' 'time employees'\n",
      " 'told cnbc' 'tonnes china' 'tracking transparency' 'traded company'\n",
      " 'transition intel' 'tree grown' 'trees growing' 'trillion august'\n",
      " 'triploid cultivars' 'uk national' 'ultimate size' 'united states'\n",
      " 'university reading' 'use company' 'user data' 'user interface'\n",
      " 'vice president' 'wall street' 'wild ancestor' 'working conditions'\n",
      " 'world largest' 'world valuable' 'year introduced' 'year round'\n",
      " 'york times']\n",
      "\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "ng_counter = CountVectorizer(max_features=300, \n",
    "                             ngram_range=(2,2), \n",
    "                             stop_words=STOP_WORDS.union({'apple', 'Apple'}))\n",
    "ng_counter.fit( fruit_sents + company_sents  )\n",
    "print(ng_counter.get_feature_names_out())\n",
    "print()\n",
    "print(len(ng_counter.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "In our original hand-built vocabulary, we had to include both \"apple\" and \"apples\".  It would have been useful to identify them as one word.\n",
    "\n",
    "This is not limited to just trailing \"s\" characters: e.g., the words \"carry\", \"carries\", \"carrying\", and \"carried\" all carry -- roughly -- the same meaning.  The process of replacing them by a common root, or **stem**, is called stemming -- the stem will not, in general, be a full word itself.\n",
    "\n",
    "There's a related process called **lemmatization**: The analog of the \"stem\" here _is_ an actual word.  After `spaCy` processes some text, the `lemma_` property of each word contains the lemmatized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carry', 'carry', 'carry', 'carry']\n",
      "['eat', 'eat', 'eat', 'eat']\n",
      "the quick brown fox jump over the lazy dog .   I can not believe it be not butter .   I try to ford the river and my unfortunate oxen die .\n"
     ]
    }
   ],
   "source": [
    "print([w.lemma_ for w in nlp('carry carries carrying carried')])\n",
    "print([w.lemma_ for w in nlp('eat eating eaten ate')])\n",
    "print(' '.join(w.lemma_ for w in nlp(\"The quick brown fox jumped over the lazy dog.  \"\n",
    "                                     \"I can't believe it's not butter.  \"\n",
    "                                     \"I tried to ford the river and my unfortunate oxen died.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We can tell our bag-of-words counters (or tf-idf) to run on lemmatized text.  This way it won't have to include both e.g., 'apple' and 'apples'.  The way to do this with `CountVectorizer` or `TfidfVectorizer` is to supply a function with the `tokenizer` option.  This function can process the text before performing the counts (or computing tf-idf values).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n' '\"' '$' '%' \"'s\" '(' ')' ',' '-' '.' '1' '10' '100' '19' '1984'\n",
      " '1985' '1997' '2' '2006' '2007' '2008' '2010' '2011' '2012' '2014' '2015'\n",
      " '2016' '2017' '2018' '2019' '2020' '2021' '3' '30' '6' ':' ';' ']'\n",
      " 'advertisement' 'allow' 'america' 'american' 'announce' 'app'\n",
      " 'application' 'april' 'attempt' 'audio' 'august' 'away' 'base' 'beat'\n",
      " 'begin' 'big' 'billion' 'board' 'brand' 'build' 'business' 'campaign'\n",
      " 'campus' 'carbon' 'cause' 'center' 'central' 'century' 'ceo' 'change'\n",
      " 'china' 'claim' 'climate' 'clone' 'color' 'come' 'company' 'computer'\n",
      " 'consumer' 'contain' 'continue' 'control' 'cook' 'corporation' 'cost'\n",
      " 'country' 'create' 'criticize' 'cultivar' 'cultivate' 'customer' 'datum'\n",
      " 'day' 'december' 'design' 'desktop' 'develop' 'development' 'device'\n",
      " 'different' 'digital' 'disease' 'early' 'eat' 'effort' 'electronic'\n",
      " 'employee' 'end' 'energy' 'europe' 'european' 'executive' 'facebook'\n",
      " 'facility' 'factory' 'feature' 'find' 'focus' 'follow' 'food' 'form'\n",
      " 'foxconn' 'fruit' 'game' 'generally' 'generation' 'genome' 'golden'\n",
      " 'good' 'google' 'government' 'green' 'group' 'grow' 'health' 'help'\n",
      " 'high' 'his' 'human' 'ii' 'imac' 'inc' 'include' 'increase' 'individual'\n",
      " 'industry' 'information' 'interface' 'introduce' 'io' 'ipad' 'iphone'\n",
      " 'ipod' 'irish' 'issue' 'itune' 'january' 'job' 'july' 'june' 'know'\n",
      " 'large' 'late' 'later' 'launch' 'lead' 'leave' 'life' 'like' 'line'\n",
      " 'local' 'location' 'logo' 'long' 'low' 'm.' 'mac' 'macbook' 'macintosh'\n",
      " 'macos' 'major' 'malus' 'manufacturer' 'manufacturing' 'march' 'market'\n",
      " 'medium' 'microsoft' 'million' 'model' 'modern' 'month' 'music' 'new'\n",
      " 'north' 'november' 'number' 'occur' 'october' 'offer' 'old' 'open'\n",
      " 'operating' 'operation' 'organic' 'original' 'pay' 'people' 'period'\n",
      " 'personal' 'pest' 'phone' 'place' 'plant' 'platform' 'point' 'policy'\n",
      " 'power' 'practice' 'price' 'privacy' 'pro' 'problem' 'produce' 'product'\n",
      " 'production' 'profit' 'program' 'project' 'provide' 'purchase' 'quality'\n",
      " 'rank' 'raw' 'reaction' 'receive' 'record' 'release' 'renewable'\n",
      " 'replace' 'report' 'research' 'result' 'retail' 'revenue' 'right'\n",
      " 'rootstock' 'run' 'sale' 'sculley' 'security' 'seed' 'sell' 'september'\n",
      " 'sequence' 'series' 'service' 'share' 'sieversii' 'significant' 'size'\n",
      " 'small' 'smartphone' 'software' 'start' 'state' 'states' 'steve'\n",
      " 'storage' 'store' 'success' 'successful' 'suicide' 'support' 'system'\n",
      " 'tax' 'technology' 'tim' 'time' 'total' 'trade' 'tree' 'u.s' 'unit'\n",
      " 'united' 'update' 'user' 'video' 'want' 'week' 'wild' 'word' 'work'\n",
      " 'worker' 'world' 'worldwide' 'wozniak' 'write' 'year' 'yield' '\\xa0' '—']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_lemma(text):\n",
    "    return [w.lemma_.lower() for w in nlp(text)]\n",
    "\n",
    "stop_words_lemma = set(tokenize_lemma(' '.join(sorted(STOP_WORDS))))\n",
    "\n",
    "ng_stem_tfidf = TfidfVectorizer(max_features=300, \n",
    "                                stop_words=stop_words_lemma.union({'apple'}),\n",
    "                                tokenizer=tokenize_lemma,\n",
    "                                token_pattern=None        # Is ignored, since tokenizer is specified\n",
    "                               )\n",
    "ng_stem_tfidf = ng_stem_tfidf.fit(fruit_sents + company_sents)\n",
    "\n",
    "ng_stem_vocab = ng_stem_tfidf.get_feature_names_out()\n",
    "print(ng_stem_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Part of speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Consider the \"Ford\" vs \"ford\" example.  As a human being, the easiest way to tell these apart is that \"Ford\" is a __noun__ while \"ford\" is a __verb__.\n",
    "\n",
    "Fortunately, `spaCy` also has a part-of-speech tagger: You give it a sentence, and it tries to tag the parts of speech (e.g., noun, verb, adjective, etc.).  The broad category is given in the `.pos_` property, while a more detailed description, using the [UPenn Treebank Tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), is in the `.tag_` property.\n",
    "\n",
    "(N.B. Nothing's perfect -- the tagger will make mistakes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "s1 = \"I tried to ford the river, and my unfortunate oxen died.\"\n",
    "s2 = \"Henry Ford built factories to facilitate the construction of the Ford automobile.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRON', 'PRP'),\n",
       " ('tried', 'VERB', 'VBD'),\n",
       " ('to', 'PART', 'TO'),\n",
       " ('ford', 'VERB', 'VB'),\n",
       " ('the', 'DET', 'DT'),\n",
       " ('river', 'NOUN', 'NN'),\n",
       " (',', 'PUNCT', ','),\n",
       " ('and', 'CCONJ', 'CC'),\n",
       " ('my', 'PRON', 'PRP$'),\n",
       " ('unfortunate', 'ADJ', 'JJ'),\n",
       " ('oxen', 'NOUN', 'NN'),\n",
       " ('died', 'VERB', 'VBD'),\n",
       " ('.', 'PUNCT', '.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(w.text, w.pos_, w.tag_) for w in nlp(s1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Henry', 'PROPN', 'NNP'),\n",
       " ('Ford', 'PROPN', 'NNP'),\n",
       " ('built', 'VERB', 'VBD'),\n",
       " ('factories', 'NOUN', 'NNS'),\n",
       " ('to', 'PART', 'TO'),\n",
       " ('facilitate', 'VERB', 'VB'),\n",
       " ('the', 'DET', 'DT'),\n",
       " ('construction', 'NOUN', 'NN'),\n",
       " ('of', 'ADP', 'IN'),\n",
       " ('the', 'DET', 'DT'),\n",
       " ('Ford', 'PROPN', 'NNP'),\n",
       " ('automobile', 'NOUN', 'NN'),\n",
       " ('.', 'PUNCT', '.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(w.text, w.pos_, w.tag_) for w in nlp(s2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Capitalization, punctuation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "There are the obvious features that we had in mind...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Building the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "*Disclaimer: This version is actually pretty bad&mdash;it uses many of the right ideas, but puts them together poorly and is being trained on fairly limited data.*\n",
    "\n",
    "Let's take all of these ideas and combine them into a function to build a classifier to do word disambiguation.  First, we create a function to retrieve text from a url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "def wikipedia_to_paragraphs(url):\n",
    "    \"\"\"\n",
    "    Retrieves a URL from wikipedia, and returns a list of paragraphs \n",
    "    (based on the 'p' html paragraph tag) \n",
    "    \"\"\"\n",
    "    files_by_url = {\n",
    "      \"http://en.wikipedia.org/wiki/Ford_(crossing)\": \"ford_crossing.txt\",\n",
    "      \"http://en.wikipedia.org/wiki/Ford\": \"ford_car.txt\",\n",
    "      \"http://en.wikipedia.org/wiki/Apple\": \"apple_fruit.txt\",\n",
    "      \"http://en.wikipedia.org/wiki/Apple_Inc.\": \"apple_inc.txt\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(\"small_data/{}\".format(files_by_url[url]), encoding='utf-8') as wiki_file:\n",
    "            soup = BeautifulSoup(wiki_file.read(), 'lxml')\\\n",
    "            .find(attrs={'id':'mw-content-text'})\n",
    "    except KeyError:\n",
    "        soup = BeautifulSoup(urlopen(url), 'lxml').find(attrs={'id':'mw-content-text'})\n",
    "    \n",
    "    # The text is littered by references like [n].  Drop them.\n",
    "    def drop_refs(s):\n",
    "        return ''.join( re.split('\\[\\d+\\]', s) )\n",
    "    \n",
    "    return [drop_refs(p.text) for p in soup.find_all('p') if p.text != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We'll then perform feature engineering in a transformer class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class AdHocFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Given a keyword (e.g., \"apple\"), will transform documents into an\n",
    "    encoding of several ad hoc features of each occurrences of the keyword:\n",
    "        - If the keyword is capitalized\n",
    "        - If it is plural\n",
    "        - If it is possessive (in the stupid sense of being followed by 's)\n",
    "        - If the keyword is a verb (e.g., for Ford vs ford)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keyword):\n",
    "        self.keyword = nlp(keyword)[0].lemma_\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.asarray([self.transform_doc(x) for x in X])\n",
    "    \n",
    "    def feature_posessive(self, doc):\n",
    "        ## N.B. spaCy will tokenize \"Apple's\" as [\"Apple\", \"'s\"]\n",
    "        hits = [i for i, word in enumerate(doc) if word.lemma_ == self.keyword]\n",
    "        return sum((i + 1) < len(doc) and doc[i+1].text == \"'s\" for i in hits)\n",
    "    \n",
    "    def transform_doc(self, row):\n",
    "        doc = nlp(row)\n",
    "        words = [word for word in doc if word.lemma_ == self.keyword]\n",
    "        return [sum(word.is_title for word in words),\n",
    "                sum(word.tag_ in ('NNS', 'NNPS') for word in words),\n",
    "                self.feature_posessive(doc),\n",
    "                sum(word.pos_ == 'VERB' for word in words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Next, we'll make our classifier. To do this, we will use a multinomial **Naive Bayes** model. Detailed information on Naive Bayes can be found in the Naive Bayes notebook, but we'll briefly describe it here: \n",
    "\n",
    "> The goal is, given a set of observed features $X_1, \\ldots, X_p$, to find the label $Y$ with the maximum conditional probability. In other words, we know what our distributions of words ($X$'s) should look like for a given genre ($Y$) from our training data, and we would like to use this information to find the genre of a body of text ($Y$) given its words ($X$'s) for new data. Bayes' theorem gives us a way to compute the latter conditional probability from the former. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "def make_classifier(base_word, meaning1, meaning2):\n",
    "    \"\"\"\n",
    "    Given\n",
    "        - a base word (e.g., \"apple\", \"ford\") that can have ambiguous meaning\n",
    "        - a pair meaning1 = (name1, url1) of a label for the first meaning, and a Wikipedia URL for it\n",
    "        - a pair meaning2 = ... for the other meaning\n",
    "    Returns a classifier that predicts the meaning\n",
    "    \"\"\"\n",
    "    name1, url1 = meaning1\n",
    "    name2, url2 = meaning2\n",
    "    para1 = wikipedia_to_paragraphs(url1)\n",
    "    para2 = wikipedia_to_paragraphs(url2)\n",
    "    minlen = min(len(para1),len(para2))\n",
    "    if len(para1) == minlen:\n",
    "        para2 = para2[:minlen]\n",
    "    else:\n",
    "        para1 = para1[:minlen]\n",
    "    \n",
    "    def tokenize_lemma(text):\n",
    "        return [w.lemma_.lower() for w in nlp(text)]\n",
    "\n",
    "    stop_words_lemma = set(tokenize_lemma(' '.join(STOP_WORDS)))\n",
    "    features = FeatureUnion([('stem_vectorizer',\n",
    "                              TfidfVectorizer(ngram_range=(1,2),\n",
    "                                              stop_words=stop_words_lemma.union({base_word}),\n",
    "                                              tokenizer=tokenize_lemma)),\n",
    "                             ('ad_hoc', AdHocFeatures(base_word))])\n",
    "    pipe = Pipeline([('features', features),\n",
    "                     ('classifier', MultinomialNB())])\n",
    "\n",
    "    # Build the training data\n",
    "    train_res  = [name1] * len(para1) + [name2] * len(para2)\n",
    "    \n",
    "    return pipe.fit(para1 + para2, train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fruit' 'company' 'fruit' 'company' 'fruit' 'fruit' 'company' 'fruit']\n"
     ]
    }
   ],
   "source": [
    "base_word = \"apple\"\n",
    "apple_fruit = (\"fruit\", \"http://en.wikipedia.org/wiki/Apple\")\n",
    "apple_company = (\"company\", \"http://en.wikipedia.org/wiki/Apple_Inc.\")\n",
    "apple_classifier = make_classifier(base_word, apple_fruit, apple_company)\n",
    "\n",
    "print(apple_classifier.predict([\n",
    "    \"I'm baking a pie with my granny smith apples.\",\n",
    "    \"I looked up the recipe on my Apple iPhone.\",\n",
    "    \"The apple pie recipe is on my desk.\",\n",
    "    \"How is Apple's stock doing?\",\n",
    "    \"I'm drinking apple juice.\",\n",
    "    \"I have three apples.\",\n",
    "    \"Steve Jobs is the CEO of apple.\",\n",
    "    \"Steve Jobs likes to eat apples.\"\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We can also do this for other classes of text documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crossing' 'company' 'company' 'crossing']\n"
     ]
    }
   ],
   "source": [
    "base_word = \"ford\"\n",
    "ford_crossing = (\"crossing\", \"http://en.wikipedia.org/wiki/Ford_(crossing)\")\n",
    "ford_company = (\"company\", \"http://en.wikipedia.org/wiki/Ford\")\n",
    "ford_classifier = make_classifier(base_word, ford_crossing, ford_company)\n",
    "\n",
    "print(ford_classifier.predict([\n",
    "    \"I tried to ford the river and my unfortunate oxen died.\",\n",
    "    \"Ford makes cars, though their quality is sometimes in dispute.\",\n",
    "    \"The Ford Mustang is an iconic automobile.\",\n",
    "    \"The river crossing was shallow, but we could not ford it.\"\n",
    "]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
